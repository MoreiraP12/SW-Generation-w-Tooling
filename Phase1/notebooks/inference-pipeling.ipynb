{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8feb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5257262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep research should be installed in the same environment as the notebook\n",
    "\n",
    "# cd ./tools/deep_research\n",
    "# pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer, pipeline\n",
    "from threading import Thread\n",
    "import json\n",
    "import time\n",
    "from deep_research.query import process_query\n",
    "from deep_research.search import search\n",
    "from deep_research.crawler import WebCrawler\n",
    "import gradio as gr\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import re\n",
    "\n",
    "class StreamingResearchPipeline:\n",
    "    def __init__(self, model_name=\"unsloth/gemma-3-4b-it\", cache_dir=\"./cache\", system_prompt_file=None):\n",
    "        \"\"\"\n",
    "        Initialize a streaming pipeline with Unsloth's Gemma3 model that can integrate\n",
    "        live research via deep_research during generation.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The Unsloth model ID\n",
    "            cache_dir (str): Directory for caching web pages\n",
    "            system_prompt_file (str): Path to a text file containing the system prompt\n",
    "        \"\"\"\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "\n",
    "        # Track timing metrics for each operation\n",
    "        self.metrics = {\n",
    "            \"model_load_time\": 0,\n",
    "            \"research_time\": 0,\n",
    "            \"total_tokens_generated\": 0,\n",
    "            \"total_searches\": 0,\n",
    "            \"successful_searches\": 0,\n",
    "            \"failed_searches\": 0\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load the model and tokenizer using FastModel\n",
    "        self.model, self.tokenizer = FastModel.from_pretrained(\n",
    "            model_name = model_name,\n",
    "            max_seq_length = 2048,  # Choose any for long context!\n",
    "            load_in_4bit = True,    # 4 bit quantization to reduce memory\n",
    "            load_in_8bit = False,   # A bit more accurate, uses 2x memory\n",
    "            full_finetuning = False # Full finetuning support\n",
    "        )\n",
    "        \n",
    "        # Apply the chat template\n",
    "        self.tokenizer = get_chat_template(\n",
    "            self.tokenizer,\n",
    "            chat_template = \"gemma-3\"\n",
    "        )\n",
    "            \n",
    "        self.metrics[\"model_load_time\"] = time.time() - start_time\n",
    "        print(f\"Model loaded successfully in {self.metrics['model_load_time']:.2f} seconds\")\n",
    "        \n",
    "        # Initialize the deep research crawler with detailed logging\n",
    "        print(\"Initializing research crawler...\")\n",
    "        self.crawler = WebCrawler(\n",
    "            cache_dir=cache_dir,\n",
    "            respect_robots=True,\n",
    "            requests_per_second=1,\n",
    "            max_depth=2,\n",
    "            max_pages_per_site=5,\n",
    "            summarization_model=\"facebook/bart-large-cnn\",\n",
    "            relevance_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "            qa_model=\"deepset/roberta-base-squad2\"\n",
    "        )\n",
    "        \n",
    "        # Store model generation parameters for transparency\n",
    "        self.generation_params = {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,  # Slightly reduced for more focused responses\n",
    "            \"top_p\": 0.92,\n",
    "            \"top_k\": 50\n",
    "        }\n",
    "        \n",
    "        # Load system prompt\n",
    "        if system_prompt_file and os.path.exists(system_prompt_file):\n",
    "            with open(system_prompt_file, 'r') as f:\n",
    "                self.system_prompt = f.read()\n",
    "            print(f\"Loaded system prompt from {system_prompt_file}\")\n",
    "        else:\n",
    "            # Improved system prompt\n",
    "            self.system_prompt = \"\"\"\n",
    "            You are an AI assistant with real-time research capabilities. When you need to search for information, \n",
    "            use the <deep_research>YOUR SEARCH QUERY</deep_research> tags. You should thoroughly research any topic \n",
    "            before providing your final answer.\n",
    "            \n",
    "            Follow these guidelines:\n",
    "            1. Use <deep_research> tags ONLY for actual web searches, not when discussing the tags themselves.\n",
    "            2. Perform as many searches as needed to gather comprehensive information.\n",
    "            3. Use specific, focused search queries rather than broad ones.\n",
    "            4. After completing all necessary research, provide your final answer between <ANSWER> and </ANSWER> tags.\n",
    "            5. Include citations to your sources in the final answer.\n",
    "            6. If answering factual questions, ensure your answer is based on the research results.\n",
    "            \n",
    "            Example usage:\n",
    "            User: \"What are the latest advancements in quantum computing?\"\n",
    "            \n",
    "            You: \"I'll research this topic for you.\n",
    "            <deep_research>latest advancements in quantum computing 2025</deep_research>\n",
    "            \n",
    "            Let me check for more specific information about quantum error correction.\n",
    "            <deep_research>quantum error correction breakthroughs 2025</deep_research>\n",
    "            \n",
    "            Now I'll look for information about quantum advantage demonstrations.\n",
    "            <deep_research>quantum computational advantage demonstrations 2025</deep_research>\n",
    "            \n",
    "            <ANSWER>\n",
    "            \n",
    "            The latest advancements in quantum computing ....\n",
    "            </ANSWER>\"\n",
    "            \"\"\"\n",
    "            print(\"Using default improved system prompt\")\n",
    "    \n",
    "    def perform_research(self, query):\n",
    "        \"\"\"\n",
    "        Execute the deep_research pipeline on a query with detailed logging\n",
    "        \n",
    "        Args:\n",
    "            query (str): The search query\n",
    "            \n",
    "        Returns:\n",
    "            dict: Research results with sources and debug information\n",
    "        \"\"\"\n",
    "        # Skip research if it appears to be discussing the tags rather than an actual query\n",
    "        if \"deep_research\" in query.lower() or \"example\" in query.lower() or len(query.strip()) < 3:\n",
    "            print(f\"Skipping research for meta-query: {query}\")\n",
    "            self.metrics[\"skipped_searches\"] = self.metrics.get(\"skipped_searches\", 0) + 1\n",
    "            return f\"\\n\\n[RESEARCH SKIPPED] Meta-query or system prompt example detected: '{query}'\\n\\n\", {\n",
    "                \"original_query\": query,\n",
    "                \"skipped\": True,\n",
    "                \"reason\": \"Meta-query or system prompt example\"\n",
    "            }\n",
    "        \n",
    "        research_start = time.time()\n",
    "        print(f\"Performing research: {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Process the query\n",
    "            query_result = process_query(query)\n",
    "            query_process_time = time.time() - research_start\n",
    "            print(f\"Query processing complete in {query_process_time:.2f}s\")\n",
    "            print(f\"Parsed query: {query_result['parsed_query']}\")\n",
    "            \n",
    "            # Search for relevant information\n",
    "            search_start = time.time()\n",
    "            search_results = search(\n",
    "                query_result['parsed_query'],\n",
    "                engines=['duckduckgo'],\n",
    "                max_results=12  # Increased for better coverage\n",
    "            )\n",
    "            search_time = time.time() - search_start\n",
    "            print(f\"Search complete in {search_time:.2f}s, found {len(search_results)} results\")\n",
    "            \n",
    "            # Log search results for visibility\n",
    "            for i, result in enumerate(search_results[:3]):\n",
    "                try:\n",
    "                    print(f\"Result {i+1}: {result.title} - {result.url}\")\n",
    "                except AttributeError:\n",
    "                    print(f\"Result {i+1}: Unable to extract title/URL from result format\")\n",
    "            \n",
    "            # Crawl and synthesize information\n",
    "            crawl_start = time.time()\n",
    "            answer = self.crawler.generate_definitive_answer(\n",
    "                query,\n",
    "                search_results=search_results\n",
    "            )\n",
    "            crawl_time = time.time() - crawl_start\n",
    "            print(f\"Crawling and synthesis complete in {crawl_time:.2f}s\")\n",
    "            \n",
    "            # Track total research time\n",
    "            total_research_time = time.time() - research_start\n",
    "            self.metrics[\"research_time\"] += total_research_time\n",
    "            self.metrics[\"total_searches\"] += 1\n",
    "            self.metrics[\"successful_searches\"] += 1\n",
    "            \n",
    "            # Format the result with original text and debug info\n",
    "            result = {\n",
    "                \"original_query\": query,\n",
    "                \"parsed_query\": query_result['parsed_query'],\n",
    "                \"answer\": answer['answer'],\n",
    "                \"sources\": answer['sources'],\n",
    "                \"metrics\": {\n",
    "                    \"query_process_time\": query_process_time,\n",
    "                    \"search_time\": search_time,\n",
    "                    \"crawl_time\": crawl_time,\n",
    "                    \"total_research_time\": total_research_time,\n",
    "                    \"num_sources\": len(answer['sources'])\n",
    "                },\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            # Format the text result\n",
    "            text_result = f\"\\n\\n[RESEARCH RESULTS for: '{query}']\\n\\n\"\n",
    "            text_result += f\"{answer['answer']}\\n\\n\"\n",
    "            text_result += \"[SOURCES]:\\n\"\n",
    "            \n",
    "            # Extract source information\n",
    "            for i, source in enumerate(answer['sources']):\n",
    "                try:\n",
    "                    title = source.title\n",
    "                    url = source.url\n",
    "                    text_result += f\"- [{i+1}] {title}: {url}\\n\"\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        title = source.get('title', source['title'] if 'title' in source else 'No title')\n",
    "                        url = source.get('url', source['url'] if 'url' in source else 'No URL')\n",
    "                        text_result += f\"- [{i+1}] {title}: {url}\\n\"\n",
    "                    except (AttributeError, KeyError, TypeError):\n",
    "                        text_result += f\"- [{i+1}] Source information unavailable\\n\"\n",
    "            \n",
    "            text_result += \"\\n\"\n",
    "            text_result += f\"[DEBUG]: Research completed in {total_research_time:.2f}s, \"\n",
    "            text_result += f\"searched {len(search_results)} pages, used {len(answer['sources'])} sources\\n\"\n",
    "            \n",
    "            return text_result, result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle exceptions during research\n",
    "            error_time = time.time() - research_start\n",
    "            error_message = f\"Error during research: {str(e)}\"\n",
    "            print(error_message)\n",
    "            \n",
    "            self.metrics[\"total_searches\"] += 1\n",
    "            self.metrics[\"failed_searches\"] += 1\n",
    "            \n",
    "            # Return error information\n",
    "            text_result = f\"\\n\\n[RESEARCH ERROR for: '{query}']\\n\\n\"\n",
    "            text_result += f\"An error occurred during research: {str(e)}\\n\"\n",
    "            text_result += f\"The model will continue generation with available information.\\n\\n\"\n",
    "            \n",
    "            result = {\n",
    "                \"original_query\": query,\n",
    "                \"error\": str(e),\n",
    "                \"error_time\": error_time,\n",
    "                \"success\": False\n",
    "            }\n",
    "            \n",
    "            return text_result, result\n",
    "    \n",
    "    def create_combined_prompt(self, user_prompt, chat_history=None):\n",
    "        \"\"\"\n",
    "        Create a chat-based prompt using the Gemma-3 format\n",
    "        \n",
    "        Args:\n",
    "            user_prompt (str): The user's input\n",
    "            chat_history (list): List of (user, assistant) message pairs\n",
    "            \n",
    "        Returns:\n",
    "            str: The formatted prompt for the model\n",
    "        \"\"\"\n",
    "        # Create messages list in the format expected by the chat template\n",
    "        messages = []\n",
    "        \n",
    "        # Add system message if provided\n",
    "        if self.system_prompt:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": self.system_prompt}]\n",
    "            })\n",
    "        \n",
    "        # Add chat history if provided\n",
    "        if chat_history:\n",
    "            for user_msg, assistant_msg in chat_history:\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": user_msg}]\n",
    "                })\n",
    "                if assistant_msg:  # Skip None responses (in case of still generating)\n",
    "                    messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": assistant_msg}]\n",
    "                    })\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "        })\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True  # Must add for generation\n",
    "        )\n",
    "        \n",
    "        # Log prompt information\n",
    "        input_ids = self.tokenizer([text], return_tensors=\"pt\").input_ids\n",
    "        prompt_tokens = input_ids.shape[1]\n",
    "        print(f\"Created prompt with {prompt_tokens} tokens\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def generate_streaming_with_research(self, prompt, chat_history=None):\n",
    "        \"\"\"\n",
    "        Generate a response with live research integration and detailed process logging\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The user input\n",
    "            chat_history (list): Optional chat history\n",
    "            \n",
    "        Returns:\n",
    "            generator: A generator yielding response tokens with research integrated\n",
    "        \"\"\"\n",
    "        # Create the formatted chat prompt\n",
    "        text = self.create_combined_prompt(prompt, chat_history)\n",
    "        \n",
    "        # Set up the text streamer\n",
    "        streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True)\n",
    "        \n",
    "        # Set up generation parameters using the exact pattern from the example\n",
    "        generation_kwargs = {\n",
    "            **self.tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
    "            \"max_new_tokens\": self.generation_params[\"max_new_tokens\"],\n",
    "            \"do_sample\": self.generation_params[\"do_sample\"],\n",
    "            \"temperature\": self.generation_params[\"temperature\"],\n",
    "            \"top_p\": self.generation_params[\"top_p\"],\n",
    "            \"top_k\": self.generation_params[\"top_k\"],\n",
    "            \"streamer\": streamer\n",
    "        }\n",
    "        \n",
    "        # Print generation information\n",
    "        print(f\"Starting generation with params: {json.dumps(self.generation_params, indent=2)}\")\n",
    "        gen_start_time = time.time()\n",
    "        \n",
    "        # Start generation in a separate thread\n",
    "        generation_thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
    "        generation_thread.start()\n",
    "        \n",
    "        # Variables for tracking tags\n",
    "        tag_buffer = \"\"\n",
    "        in_tag = False\n",
    "        research_query = \"\"\n",
    "        token_count = 0\n",
    "        \n",
    "        # Variables for tracking answer tags\n",
    "        in_answer = False\n",
    "        final_answer = \"\"\n",
    "        \n",
    "        # Debug data for UI display\n",
    "        debug_data = {\n",
    "            \"token_count\": 0,\n",
    "            \"token_rate\": 0,\n",
    "            \"research_queries\": [],\n",
    "            \"research_times\": [],\n",
    "            \"has_final_answer\": False\n",
    "        }\n",
    "        \n",
    "        # Stream tokens and look for research tags\n",
    "        generated_text = \"\"\n",
    "        start_time = time.time()\n",
    "        for new_text in streamer:\n",
    "            current_time = time.time()\n",
    "            token_count += 1\n",
    "            debug_data[\"token_count\"] = token_count\n",
    "            elapsed = current_time - start_time\n",
    "            if elapsed > 0:\n",
    "                debug_data[\"token_rate\"] = token_count / elapsed\n",
    "            \n",
    "            # Format debug info to show in UI\n",
    "            debug_str = f\"[REALTIME DEBUG] Tokens: {token_count} | Rate: {debug_data['token_rate']:.1f} t/s\"\n",
    "            if debug_data[\"research_queries\"]:\n",
    "                debug_str += f\" | Searches: {len(debug_data['research_queries'])}\"\n",
    "            if debug_data[\"has_final_answer\"]:\n",
    "                debug_str += f\" | Final Answer: Found\"\n",
    "                \n",
    "            # Include the debug info in what we yield\n",
    "            yield debug_str + \"\\n\"\n",
    "            \n",
    "            generated_text += new_text\n",
    "            tag_buffer += new_text\n",
    "            \n",
    "            # Format token information for UI\n",
    "            token_info = f\"[TOKEN] '{new_text}'\"\n",
    "            yield token_info + \"\\n\"\n",
    "            \n",
    "            # Check for opening research tag\n",
    "            if not in_tag and \"<deep_research>\" in tag_buffer:\n",
    "                in_tag = True\n",
    "                tag_start_index = tag_buffer.find(\"<deep_research>\")\n",
    "                # Yield everything before the tag\n",
    "                yield f\"[TAG DETECTED] Opening research tag\\n\"\n",
    "                # Reset the buffer to just track what's inside the tag\n",
    "                tag_buffer = tag_buffer[tag_start_index + len(\"<deep_research>\"):]\n",
    "            \n",
    "            # Check for closing research tag when we're inside a tag\n",
    "            elif in_tag and \"</deep_research>\" in tag_buffer:\n",
    "                in_tag = False\n",
    "                research_query = tag_buffer[:tag_buffer.find(\"</deep_research>\")]\n",
    "                \n",
    "                # Yield the query and that we're researching\n",
    "                yield f\"[RESEARCH STARTED] Query: '{research_query}'\\n\"\n",
    "                \n",
    "                # Perform the research\n",
    "                query_start_time = time.time()\n",
    "                text_results, research_results = self.perform_research(research_query)\n",
    "                query_duration = time.time() - query_start_time\n",
    "                \n",
    "                # Update debug data\n",
    "                debug_data[\"research_queries\"].append(research_query)\n",
    "                debug_data[\"research_times\"].append(query_duration)\n",
    "                \n",
    "                # Yield research status\n",
    "                if research_results.get(\"success\", False):\n",
    "                    yield f\"[RESEARCH COMPLETED] in {query_duration:.2f}s\\n\"\n",
    "                else:\n",
    "                    yield f\"[RESEARCH FAILED] in {query_duration:.2f}s: {research_results.get('error', 'Unknown error')}\\n\"\n",
    "                \n",
    "                # Yield research data for UI\n",
    "                try:\n",
    "                    research_meta = {\n",
    "                        \"query\": research_query,\n",
    "                        \"duration\": query_duration,\n",
    "                        \"success\": research_results.get(\"success\", False)\n",
    "                    }\n",
    "                    \n",
    "                    if research_results.get(\"success\", False):\n",
    "                        research_meta[\"num_sources\"] = len(research_results[\"sources\"])\n",
    "                        research_meta[\"sources\"] = []\n",
    "                        for s in research_results[\"sources\"][:3]:\n",
    "                            try:\n",
    "                                research_meta[\"sources\"].append({\"title\": s.title, \"url\": s.url})\n",
    "                            except AttributeError:\n",
    "                                try:\n",
    "                                    research_meta[\"sources\"].append({\n",
    "                                        \"title\": s.get(\"title\", \"No title\"),\n",
    "                                        \"url\": s.get(\"url\", \"No URL\")\n",
    "                                    })\n",
    "                                except (AttributeError, KeyError, TypeError):\n",
    "                                    research_meta[\"sources\"].append({\"title\": \"Source info unavailable\", \"url\": \"\"})\n",
    "                    \n",
    "                    yield f\"[RESEARCH DETAILS] {json.dumps(research_meta, indent=2)}\\n\"\n",
    "                except Exception as e:\n",
    "                    yield f\"[RESEARCH DETAILS ERROR] Failed to generate research details: {str(e)}\\n\"\n",
    "                \n",
    "                # Yield the actual research results\n",
    "                yield text_results\n",
    "                \n",
    "                # Reset the buffer\n",
    "                tag_buffer = \"\"\n",
    "            \n",
    "            # Check for opening answer tag\n",
    "            elif not in_answer and \"<ANSWER>\" in generated_text[-20:]:\n",
    "                in_answer = True\n",
    "                debug_data[\"has_final_answer\"] = True\n",
    "                yield f\"[FINAL ANSWER STARTED]\\n\"\n",
    "                \n",
    "            # Check for closing answer tag\n",
    "            elif in_answer and \"</ANSWER>\" in generated_text[-20:]:\n",
    "                in_answer = False\n",
    "                yield f\"[FINAL ANSWER COMPLETED]\\n\"\n",
    "                \n",
    "                # Extract the final answer\n",
    "                answer_pattern = r\"<ANSWER>(.*?)</ANSWER>\"\n",
    "                answer_matches = re.findall(answer_pattern, generated_text, re.DOTALL)\n",
    "                if answer_matches:\n",
    "                    final_answer = answer_matches[-1].strip()\n",
    "                    yield f\"[EXTRACTED ANSWER] Length: {len(final_answer)} characters\\n\"\n",
    "            \n",
    "            # Normal streaming when not at a tag boundary\n",
    "            elif not in_tag:\n",
    "                tag_buffer = tag_buffer[-30:]  # Keep a sliding window for tag detection\n",
    "        \n",
    "        # Generation complete - yield final stats\n",
    "        total_gen_time = time.time() - gen_start_time\n",
    "        self.metrics[\"total_tokens_generated\"] += token_count\n",
    "        \n",
    "        # Extract the final answer if it exists but wasn't caught during streaming\n",
    "        if not final_answer:\n",
    "            answer_pattern = r\"<ANSWER>(.*?)</ANSWER>\"\n",
    "            answer_matches = re.findall(answer_pattern, generated_text, re.DOTALL)\n",
    "            if answer_matches:\n",
    "                final_answer = answer_matches[-1].strip()\n",
    "        \n",
    "        final_stats = {\n",
    "            \"total_tokens\": token_count,\n",
    "            \"generation_time\": total_gen_time, \n",
    "            \"tokens_per_second\": token_count / total_gen_time if total_gen_time > 0 else 0,\n",
    "            \"research_queries\": len(debug_data[\"research_queries\"]),\n",
    "            \"total_research_time\": sum(debug_data[\"research_times\"]),\n",
    "            \"average_research_time\": sum(debug_data[\"research_times\"]) / len(debug_data[\"research_times\"]) if debug_data[\"research_times\"] else 0,\n",
    "            \"has_final_answer\": bool(final_answer)\n",
    "        }\n",
    "        \n",
    "        yield f\"\\n\\n[GENERATION COMPLETE]\\n{json.dumps(final_stats, indent=2)}\\n\"\n",
    "        \n",
    "        # If there's a final answer, yield it separately for easy access\n",
    "        if final_answer:\n",
    "            yield f\"\\n\\n[FINAL FORMATTED ANSWER]\\n{final_answer}\\n\"\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Return the current pipeline metrics as a formatted string\"\"\"\n",
    "        return json.dumps(self.metrics, indent=2)\n",
    "    \n",
    "    def chat(self, user_prompt, chat_history=None):\n",
    "        \"\"\"\n",
    "        Convenience method for chat-style usage\n",
    "        \n",
    "        Args:\n",
    "            user_prompt (str): The user's message\n",
    "            chat_history (list): Optional list of (user, assistant) message pairs\n",
    "            \n",
    "        Returns:\n",
    "            str: The complete response with research integrated\n",
    "        \"\"\"\n",
    "        response = \"\"\n",
    "        for token in self.generate_streaming_with_research(user_prompt, chat_history):\n",
    "            response += token\n",
    "        return response\n",
    "\n",
    "\n",
    "# Improved Gradio interface with two panels:\n",
    "# 1. Chat interface for normal interaction\n",
    "# 2. Debug panel showing real-time information\n",
    "def create_gradio_interface():\n",
    "    # Initialize the pipeline\n",
    "    pipeline = StreamingResearchPipeline(\n",
    "        model_name=\"unsloth/gemma-3-4b-it\",\n",
    "        cache_dir=\"./cache\",\n",
    "        system_prompt_file=None\n",
    "    )\n",
    "\n",
    "    # Initialize chat history\n",
    "    history = []\n",
    "    \n",
    "    # Store debug information\n",
    "    debug_history = []\n",
    "    \n",
    "    # Store final answers for each query\n",
    "    final_answers = {}\n",
    "\n",
    "    def user(message, history):\n",
    "        # Return the user's message to display it\n",
    "        debug_history.clear()  # Reset debug for new query\n",
    "        return message, history + [[message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        # Extract the user's message\n",
    "        user_message = history[-1][0]\n",
    "        \n",
    "        # Convert previous history to the format expected by the pipeline\n",
    "        prev_history = history[:-1] if len(history) > 1 else None\n",
    "        \n",
    "        # For storing the actual response vs debug info\n",
    "        actual_response = \"\"\n",
    "        current_debug = \"\"\n",
    "        final_answer = \"\"\n",
    "        \n",
    "        # Track if we're currently inside the answer block\n",
    "        in_answer_block = False\n",
    "        \n",
    "        # Generate the response with research integration\n",
    "        for token in pipeline.generate_streaming_with_research(user_message, prev_history):\n",
    "            # Check for final formatted answer\n",
    "            if token.startswith(\"\\n\\n[FINAL FORMATTED ANSWER]\"):\n",
    "                final_answer = token.split(\"\\n\", 3)[3].strip()\n",
    "                # Don't add this to the debug or response\n",
    "                continue\n",
    "                \n",
    "            # Check if this is a debug/status line\n",
    "            if token.startswith(\"[\"):\n",
    "                # Store the debug info to display in debug panel\n",
    "                debug_history.append(token)\n",
    "                current_debug = \"\\n\".join(debug_history[-20:])  # Keep last 20 debug lines\n",
    "                \n",
    "                # If it's a research result, add it to the actual response\n",
    "                if token.startswith(\"[RESEARCH RESULTS\") or token.startswith(\"[SOURCES\"):\n",
    "                    actual_response += token\n",
    "                elif token.startswith(\"[FINAL ANSWER STARTED]\"):\n",
    "                    in_answer_block = True\n",
    "                elif token.startswith(\"[FINAL ANSWER COMPLETED]\"):\n",
    "                    in_answer_block = False\n",
    "            else:\n",
    "                # Normal text token - add to the response\n",
    "                actual_response += token\n",
    "            \n",
    "            # Update both panels\n",
    "            history[-1][1] = actual_response\n",
    "            yield history, current_debug\n",
    "        \n",
    "        # Store the final answer for this query\n",
    "        if final_answer:\n",
    "            final_answers[user_message] = final_answer\n",
    "            \n",
    "            # If we found a final answer, replace the response with it\n",
    "            # This ensures the user sees a clean answer without all the debug info\n",
    "            clean_response = f\"I researched your question thoroughly.\\n\\n{final_answer}\"\n",
    "            history[-1][1] = clean_response\n",
    "        \n",
    "        # Final stats - add pipeline metrics\n",
    "        debug_history.append(\"\\n[PIPELINE METRICS]\")\n",
    "        debug_history.append(pipeline.get_metrics())\n",
    "        current_debug = \"\\n\".join(debug_history[-20:])\n",
    "        \n",
    "        # Final yield to ensure everything is displayed\n",
    "        yield history, current_debug\n",
    "\n",
    "    with gr.Blocks(css=\"footer {visibility: hidden}\") as demo:\n",
    "        gr.Markdown(\"# üîç Gemma3 with Deep Research Integration\")\n",
    "        gr.Markdown(\"\"\"This enhanced demo shows the real-time inner workings of Unsloth's optimized Gemma3 model with research capabilities.\n",
    "        \n",
    "        The model streams every token as it's generated and shows you exactly when it searches for information using `<deep_research>` tags.\n",
    "        It then provides the final researched answer between `<ANSWER>` tags.\n",
    "        \n",
    "        Try asking about current events, recent news, or specific factual information!\n",
    "        \n",
    "        **The right panel shows you real-time debug information including:**\n",
    "        - Each token as it's generated\n",
    "        - When research is triggered and completed\n",
    "        - Source information and metrics\n",
    "        - Performance statistics\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=6):\n",
    "                chatbot = gr.Chatbot([], elem_id=\"chatbot\", height=600)\n",
    "                with gr.Row():\n",
    "                    msg = gr.Textbox(placeholder=\"Ask me anything...\", show_label=False)\n",
    "                    clear = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            with gr.Column(scale=4):\n",
    "                debug_panel = gr.Textbox(\n",
    "                    value=\"Debug information will appear here during generation...\",\n",
    "                    label=\"Real-time Debug Information\",\n",
    "                    lines=30,\n",
    "                    max_lines=30\n",
    "                )\n",
    "        \n",
    "        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "            bot, [chatbot], [chatbot, debug_panel]\n",
    "        )\n",
    "        clear.click(lambda: [], None, [chatbot, debug_panel], queue=False)\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        ### System Information\n",
    "        - **Model**: Unsloth's optimized Gemma-3-4b with custom research capabilities\n",
    "        - **Hardware**: Running on CUDA for acceleration\n",
    "        - **Research**: Uses DuckDuckGo search and fetches/summarizes web content\n",
    "        - **Format**: Final answers are provided within <ANSWER> tags\n",
    "        \"\"\")\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
